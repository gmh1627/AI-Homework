import torch
import torch.nn.functional as F

# Assuming x is your tensor
x = torch.tensor([[0.0072, 0.0071, 0.0068, 0.0062, 0.0000, 0.0075, 0.0069, 0.0000, 0.0068,
         0.0063, 0.0000, 0.0077, 0.0077, 0.0081, 0.0068, 0.0068, 0.0073, 0.0000,
         0.0065, 0.0067, 0.0068, 0.0075, 0.0074, 0.0065, 0.0000, 0.0068, 0.0067,
         0.0070, 0.0070, 0.0000, 0.0067, 0.0065, 0.0066, 0.0065, 0.0071, 0.0065,
         0.0073, 0.0075, 0.0000, 0.0069, 0.0000, 0.0076, 0.0000, 0.0000, 0.0072,
         0.0065, 0.0075, 0.0000, 0.0067, 0.0073, 0.0067, 0.0000, 0.0079, 0.0000,
         0.0066, 0.0073, 0.0074, 0.0072, 0.0071, 0.0078, 0.0072, 0.0078, 0.0000,
         0.0000, 0.0066, 0.0000, 0.0065, 0.0071, 0.0072, 0.0068, 0.0071, 0.0069,
         0.0069, 0.0068, 0.0067, 0.0066, 0.0067, 0.0069, 0.0059, 0.0063, 0.0000,
         0.0000, 0.0072, 0.0068, 0.0068, 0.0068, 0.0068, 0.0000, 0.0065, 0.0077,
         0.0065, 0.0068, 0.0071, 0.0077, 0.0072, 0.0063, 0.0069, 0.0000, 0.0000,
         0.0069, 0.0073, 0.0073, 0.0062, 0.0068, 0.0073, 0.0067, 0.0000, 0.0069,
         0.0000, 0.0068, 0.0073, 0.0071, 0.0068, 0.0066, 0.0000, 0.0000, 0.0064,
         0.0000, 0.0064, 0.0075, 0.0000, 0.0066, 0.0066, 0.0074, 0.0072, 0.0061,
         0.0000, 0.0067, 0.0066, 0.0066, 0.0071, 0.0065, 0.0067, 0.0063, 0.0070,
         0.0076, 0.0000, 0.0000, 0.0063, 0.0000, 0.0064, 0.0076, 0.0000, 0.0071]],
       device='cuda:0', requires_grad=True)

# Create a mask for non-zero elements
non_zero_mask = x != 0

# Set zero elements to a very small negative number
x_adjusted = torch.where(non_zero_mask, x, torch.tensor(-1e10).to(x.device))

# Apply softmax to the adjusted tensor
softmax_x_adjusted = F.softmax(x_adjusted, dim=1)

# Set the originally zero elements back to zero
softmax_x_final = torch.where(non_zero_mask, softmax_x_adjusted, x)

print(softmax_x_final)